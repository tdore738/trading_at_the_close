{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84149c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random as rand\n",
    "from scipy import *\n",
    "import time as T\n",
    "import gtda\n",
    "import plotly\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "init_notebook_mode(connected=True)\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.time_series import TakensEmbedding\n",
    "from gtda.time_series import takens_embedding_optimal_parameters\n",
    "\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fda4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../train.csv\"\n",
    "ALL_DATA = pd.read_csv(path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6bd98",
   "metadata": {},
   "source": [
    "# Proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0dabb2",
   "metadata": {},
   "source": [
    "Here, we'll try a proof of conept using persistent homology and time series prediction.   \n",
    "- We'll use that data at hand to try and predict the last value of the Target time series (obviously without using Target itself, since we'll need that\n",
    "- We'll do this by treating every date and stock as different inputs (i.e. same stock on different day and different stock on same days are equally different inputs)\n",
    "- We will perform sliding-window-delay-embedding (SWDE) on each input (time series) and use that to solve a **classification problem** on the final point: split the real line into bins, which bin does the point end up in?\n",
    "- Then, armed with bin probabilities of the final point, we use those probabilities plus LSTM or Regressor of the final minute or 2 (6 to 12 data points $\\times$ columns $\\times$ inputs) to find a precise final answer\n",
    "- Finally, we can try and assess which columns are most important by removing columns and seeing how much the answer changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e50a84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_PoC(df):\n",
    "    \"\"\"\n",
    "    Absolute bare minimal preprocessing for the Optiver dataset\n",
    "    \n",
    "    stolen from Jay ;)  (modified a bit)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fill NaNs in near_price and far_price with reference_price\n",
    "#     df['near_price'] = df['near_price'].fillna(df['reference_price'])\n",
    "#     df['far_price'] = df['far_price'].fillna(df['reference_price'])\n",
    "#     df['wap'] = df['wap'].fillna(df['reference_price'])\n",
    "\n",
    "    # Instead of the above, after a good look, there are only 4 stocks on 4 particular days missing:\n",
    "    # (stock,date) -- (131,35); (101,328); (158,388); (19,438)\n",
    "    # and for some reason stock 101 and 19 have columns filled in for 'Target', 'bid_size', and 'ask_size'\n",
    "    # but notthing else.. strange but we'll treat that as not enough information for now\n",
    "    \n",
    "    # !! So, in that case we will actually just DROP ALL 'NaN'  !!\n",
    "    # But, first we need to drop near and far price columns, we won't use these in this part of the training\n",
    "    # only later during the precision part\n",
    "        \n",
    "    df = df.drop(columns=['near_price','far_price']).dropna()\n",
    "    \n",
    "    # I think it is also good to re-index in a more intuitive way.. right now the indexing from slow to fast is:\n",
    "    # (  date_id, seconds_in_bucket, stock_id  ), but since we are doing time series, I think it makes sense\n",
    "    # to swap the speed of seconds and stock, so that i.loc[55*(x-1):55*x] indexes a stock on a given day\n",
    "    df = (df\n",
    "          .set_index(['date_id','stock_id','seconds_in_bucket'])\n",
    "          .sort_index(level=['date_id','stock_id','seconds_in_bucket'], sort_remaining=False))\n",
    "    \n",
    "    # Drop row_id and time_id, not needed for training\n",
    "    df = df.drop(['row_id', 'time_id'], axis=1)\n",
    "    # And, in this case, we make sure to only select on the last data point of each time series input\n",
    "    targets = df[['target']].loc[pd.IndexSlice[:, :, 540]]\n",
    "    df = df.drop(['target'], axis=1)\n",
    "    \n",
    "    return df, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57bf01",
   "metadata": {},
   "source": [
    "### Process data and split training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1a8a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed, y_processed = preprocess_PoC(ALL_DATA)\n",
    "training_size_from_date = 450 # let's train on first 450 dates, validate on the rest\n",
    "\n",
    "X_train = X_processed.loc[pd.IndexSlice[0:training_size_from_date, :, :]]\n",
    "y_train_unbinned = y_processed.loc[pd.IndexSlice[0:training_size_from_date, :, :]]\n",
    "\n",
    "X_valid = X_processed.loc[pd.IndexSlice[training_size_from_date:, :, :]]\n",
    "y_valid_unbinned = y_processed.loc[pd.IndexSlice[training_size_from_date:, :, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cce564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, dim = takens_embedding_optimal_parameters(\n",
    "    df[col].to_numpy(), max_time_delay=100, max_dimension=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
