{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f353f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random as rand\n",
    "from scipy import *\n",
    "import time as T\n",
    "import gtda\n",
    "import plotly\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "init_notebook_mode(connected=True)\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from gtda.plotting import plot_diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83f591",
   "metadata": {},
   "source": [
    "# Testing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925edbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../train.csv\"\n",
    "ALL_DATA = pd.read_csv(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e708c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_PoC(df):\n",
    "       \n",
    "    df = df.drop(columns=['near_price','far_price']).dropna()\n",
    "\n",
    "    df = (df\n",
    "          .set_index(['date_id','stock_id','seconds_in_bucket'])\n",
    "          .sort_index(level=['date_id','stock_id','seconds_in_bucket'], sort_remaining=False))\n",
    "    \n",
    "    # Drop row_id and time_id, not needed for training\n",
    "    df = df.drop(['row_id', 'time_id'], axis=1)\n",
    "    # And, in this case, we select every 10th of each time series input\n",
    "    targets = df[['target']]#.loc[pd.IndexSlice[:, :, ::10]]\n",
    "    df = df.drop(['target'], axis=1)\n",
    "    \n",
    "    return df, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2272a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed, y_processed = preprocess_PoC(ALL_DATA)\n",
    "%reset_selective ALL_ASSETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wide = X_processed.unstack('stock_id')\n",
    "x_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wide = y_processed['target'].unstack('stock_id')\n",
    "# y_wide.xs(0,level='date_id')\n",
    "y_wide[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa35854",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wide.groupby(level='date_id').nth(0).iloc[0:50,60:71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f361c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wide.groupby(level='date_id').nth(54).corr().iloc[85:95,85:95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db712d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(~np.isfinite(y_wide.groupby(level='date_id').nth(70).corr().to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940b726",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for dt, subdf in y_wide.groupby(level='date_id'):\n",
    "for i in range(0,63,9):\n",
    "    subdf = y_wide.groupby(level='date_id').nth(i)\n",
    "    f = plt.figure(figsize=(12, 12))\n",
    "    plt.matshow(subdf.corr(), fignum=f.number)\n",
    "    plt.xticks(range(0,subdf.select_dtypes(['number']).shape[1],10), subdf.select_dtypes(['number']).columns[::10], fontsize=14, rotation=45)\n",
    "    plt.yticks(range(0,subdf.select_dtypes(['number']).shape[1],10), subdf.select_dtypes(['number']).columns[::10], fontsize=14)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title(f'Correlation Matrix at time {i*10}', fontsize=16);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wide.xs(500,level='seconds_in_bucket').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6a106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f1c66a6",
   "metadata": {},
   "source": [
    "# Older Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_and_wap = pd.read_csv('../train.csv',delimiter=',',usecols=['stock_id','seconds_in_bucket','wap','date_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Sid = 100\n",
    "my_Did = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_stock = id_and_wap[np.bitwise_and(id_and_wap['stock_id']==my_Sid, id_and_wap['date_id']==my_Did)]\n",
    "my_stock = id_and_wap[id_and_wap['stock_id']==my_Sid]\n",
    "time = my_stock['seconds_in_bucket']\n",
    "print(time.size//481)\n",
    "time = time.values.reshape(481,time.size//481)\n",
    "wap = my_stock['wap'].values.reshape(481,time.size//481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db072025",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bddf3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_time = time[0]\n",
    "concat_wap = wap[0]\n",
    "prevT = concat_time[-1]\n",
    "for t,w in zip(time[1:],wap[1:]):\n",
    "    newT = t+prevT\n",
    "    concat_time = np.append(concat_time,newT)\n",
    "    concat_wap = np.append(concat_wap,w)\n",
    "    prevT = concat_time[-1]\n",
    "plt.plot(concat_time,concat_wap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edafefe",
   "metadata": {},
   "source": [
    "# Let's already try some TDA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b65441",
   "metadata": {},
   "source": [
    "## for a baseline, let's look at some Black-Scholes time series and then maybe some Heston or Heston-Nandi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ad7e1",
   "metadata": {},
   "source": [
    "## Black-Scholes (log returns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_path_BS(S0, t, r, mu, sigma, n_paths, n_steps, log=False):\n",
    "    '''\n",
    "    Generation of stock paths following Geometeric Brownian motion\n",
    "    \n",
    "    Inputs:\n",
    "    S0 (float): initial stock value\n",
    "    t (float): time interval of stock path movements in years\n",
    "    r (float): risk-free interest rate\n",
    "    mu (float): drift of log-returns\n",
    "    sigma (float): volatility\n",
    "    n_paths (int): number of stock paths\n",
    "    n_steps (float): number of steps in each stock path\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Simulated stock paths\n",
    "    '''\n",
    "    \n",
    "    #Noise in volatility\n",
    "    noise = np.random.normal(0,1,size = (n_paths, n_steps-1))\n",
    "        \n",
    "    #Time increment between each step, needs to match proper shape\n",
    "    dt = t/n_steps\n",
    "    \n",
    "    #log-returns between each step\n",
    "    increments = (mu + r - .5*sigma**2)*dt + sigma*np.sqrt(dt)*noise\n",
    "    \n",
    "    #Cumulative log-returns at each step\n",
    "    log_returns = np.cumsum(increments, axis = 1)\n",
    "    \n",
    "    #paths\n",
    "    paths = S0*np.exp(log_returns) if not log else log_returns\n",
    "    \n",
    "    #Adjoint initial value S0 at start of each simulated path\n",
    "    if not log:\n",
    "        paths = np.insert(paths, 0, S0, axis = 1)\n",
    "    else:\n",
    "    #S0 = 1 for log return paths\n",
    "        paths = np.insert(paths, 0, 1, axis = 1)\n",
    "    \n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886e4d6",
   "metadata": {},
   "source": [
    "Let's construct something close to 30 point clouds in 3 dimensions with 100 points, so we can follow allowing with the giotto tutorial. However, we will not construct a random point cloud, but instead construct those paramters with 3 PATHS, AND 990 TIME STEPS. This will allow us to bin it into 30 point clouds of 99 points each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#S0 is meaningless for log returns\n",
    "Npaths = 3\n",
    "clouds = 10\n",
    "points_per_cloud_per_path = 100\n",
    "\n",
    "ALL_PATHS = stock_path_BS(100, 10, 0.01, 0.02, 0.3, Npaths, clouds*points_per_cloud_per_path,log=True)\n",
    "analyze_differences = True\n",
    "if analyze_differences:\n",
    "    for i in range(ALL_PATHS.shape[0]):\n",
    "        ALL_PATHS[i][0] = 0\n",
    "        ALL_PATHS[i][1:] = ALL_PATHS[i][1:] - ALL_PATHS[i][:-1]\n",
    "\n",
    "#Create 30 point cloud batches\n",
    "BS_point_clouds = np.reshape(ALL_PATHS,(Npaths,clouds,points_per_cloud_per_path))\n",
    "BS_point_clouds = BS_point_clouds.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4458a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BS_point_clouds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for j in range(Npaths):\n",
    "    plt.plot(range(points_per_cloud_per_path),BS_point_clouds[i,:,j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccae26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_point_clouds[9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ff911",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(BS_point_clouds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])\n",
    "diagrams_BS = VR.fit_transform(BS_point_clouds)\n",
    "diagrams_BS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53979ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "i = 5\n",
    "plot_diagram(diagrams_BS[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7be08e",
   "metadata": {},
   "source": [
    "# Heston-Nandi Garch 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c52308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_path_heston_nandi(S0, h0, t, mu, r, Var_mean, beta, alpha, gamma, n_paths, n_steps,scale=1, lam=-1/2,\n",
    "                           log=False):\n",
    "    '''\n",
    "    Generation of custom stock paths following Heston-Nandi Garch (1,1) \n",
    "    \n",
    "    ((  Heston, Steven L. and Nandi, Saikat, A Closed-Form GARCH Option Valuation Model.\n",
    "    Available at SSRN: https://ssrn.com/abstract=210009 or http://dx.doi.org/10.2139/ssrn.210009  ))\n",
    "    \n",
    "    This is a GARCH(1,1) process, where the mean (Stock price) dynamics are given by something\n",
    "    structurally similar to Black-Scholes model, Heston model, etc\n",
    "    \n",
    "    Inputs: (loosley following notation from paper)\n",
    "    -S0 (float): inital stock value\n",
    "    -h0 (float): initial variance\n",
    "    -mu (float): drift of log returns\n",
    "    -r (float): risk free rate\n",
    "    - Var_mean (float): Mean of variance, used to calulate omega, the GARCH parameter\n",
    "    -beta (float): GARCH mean reversion parameter for variance\n",
    "    -alpha (float): GARCH error influence parameter (moving average) for variance\n",
    "    -lam (float): lambda in the Heston-Nandi paper, \"market price of risk\", set to -1/2 for risk neutral\n",
    "    -gamma (float): \"asymmetric influence\", extra GARCH parameter which influeces moving average for variance\n",
    "    \n",
    "    - S_mean (float): Mean of stock price, used to calculate mu, the ARMA parameter\n",
    "    - Var_mean (float): Mean of variance, used to calulate omega, the GARCH parameter\n",
    "    \n",
    "    -n_paths (int): number of stock paths\n",
    "    -n_steps (float): number of steps in each stock path\n",
    "    \n",
    "    scale (int): an intraday scale that helps reduce flatness of S_T, can be seen as reducing dt\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    Simulated stock paths\n",
    "    Simulated volatility paths\n",
    "    '''\n",
    "    \n",
    "    n_steps = n_steps*scale - 1\n",
    "    #GARCH parameters also need to be rescaled\n",
    "    alpha /= scale\n",
    "    beta /= scale\n",
    "    \n",
    "    #converting from mean varaince to haston-nandi GARCH omega\n",
    "    omega = Var_mean*(1 - beta - alpha*(1 + gamma*gamma))\n",
    "    \n",
    "    #following notation of the paper, z is the noise at time t\n",
    "    z = np.random.normal(0,1,size = (n_paths,n_steps))\n",
    "\n",
    "    #set arrays\n",
    "    h_T = np.zeros((n_paths, n_steps+1))\n",
    "    S_T = np.zeros((n_paths, n_steps+1))\n",
    "    h_T[:,0] = h0\n",
    "    S_T[:,0] = S0 if not log else 0\n",
    "    \n",
    "    dt = t/n_steps\n",
    "    if not log:\n",
    "        for i in range(n_steps):\n",
    "            h_prev, S_prev = h_T[:,i], S_T[:,i]\n",
    "\n",
    "            h_T[:,i+1] = omega + beta*h_prev + alpha*(z[:,i] - gamma*np.sqrt(h_prev))**2\n",
    "            var_T = h_T[:,i+1]\n",
    "            #KEY DIFFERENCE BETWEEN THIS AND HESTON: S-T CALCULATED FROM **CURRENT** VOLATILITY\n",
    "            S_T[:,i+1] = S_prev*np.exp((mu + r - lam*var_T)*dt + np.sqrt(var_T*dt)*z[:,i])\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            h_prev, S_prev = h_T[:,i], S_T[:,i]\n",
    "\n",
    "            h_T[:,i+1] = omega + beta*h_prev + alpha*(z[:,i] - gamma*np.sqrt(h_prev))**2\n",
    "            var_T = h_T[:,i+1]\n",
    "            #KEY DIFFERENCE BETWEEN THIS AND HESTON: S_T CALCULATED FROM **CURRENT** VOLATILITY\n",
    "            S_T[:,i+1] = (mu + r - lam*var_T)*dt + np.sqrt(var_T*dt)*z[:,i]    \n",
    "        \n",
    "    return S_T[:,::scale], h_T[:,::scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 100\n",
    "t = 1\n",
    "r = 0.039\n",
    "mu = 0\n",
    "h0 = 0.3\n",
    "Var_mean = 0.3\n",
    "beta = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.1\n",
    "\n",
    "Npaths = 30\n",
    "clouds = 30\n",
    "points_per_cloud_per_path = 5\n",
    "scale = 1\n",
    "\n",
    "S_T, nu_T = stock_path_heston_nandi(S0, h0, t, mu, r, Var_mean, beta, alpha, gamma, Npaths,\\\n",
    "                                    points_per_cloud_per_path*clouds, scale,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_differences = True\n",
    "if analyze_differences:\n",
    "    for i in range(S_T.shape[0]):\n",
    "        S_T[i][0] = 0\n",
    "        S_T[i][1:] = S_T[i][1:] - S_T[i][:-1]\n",
    "        \n",
    "#Create 30 point cloud batches\n",
    "HN_point_clouds = np.reshape(S_T,(Npaths,clouds,points_per_cloud_per_path))\n",
    "HN_point_clouds = HN_point_clouds.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=20\n",
    "for j in range(Npaths):\n",
    "    plt.plot(range(points_per_cloud_per_path),HN_point_clouds[i,:,j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407aa42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HN_point_clouds[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(HN_point_clouds[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef50e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])\n",
    "diagrams_HN = VR.fit_transform(HN_point_clouds)\n",
    "diagrams_HN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa77373",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(diagrams_HN[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e24ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import PearsonDissimilarity\n",
    "from gtda.diagrams import Amplitude\n",
    "PD = PearsonDissimilarity()\n",
    "\n",
    "X_pd = PD.fit_transform(HN_point_clouds)\n",
    "VR = VietorisRipsPersistence(metric=\"precomputed\",homology_dimensions=[0, 1, 2])\n",
    "X_vr = VR.fit_transform(X_pd)  # \"precomputed\" required on dissimilarity data\n",
    "Ampl = Amplitude()\n",
    "X_a = Ampl.fit_transform(X_vr)\n",
    "X_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301ec28",
   "metadata": {},
   "source": [
    "# Let's try  on teh data?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a53c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of this, let's try on one stock at a time\n",
    "# let's analyze changes instead of raw values (since we don't know the scale anyway)\n",
    "# finally, let's decide on some window length, i guess one window per day (54 points per window per stock)\n",
    "# will do for now, and if it's too small, we can try including multiple days into a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0406d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_and_wap = pd.read_csv('../train.csv',delimiter=',',usecols=['stock_id','date_id','seconds_in_bucket','wap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_stock = id_and_wap[np.bitwise_and(id_and_wap['stock_id']==my_Sid, id_and_wap['date_id']==my_Did)]\n",
    "my_stock = id_and_wap[id_and_wap['stock_id']==100].drop(columns='stock_id')\n",
    "# time = my_stock['seconds_in_bucket'].values.reshape(481,time.size//481)\n",
    "# wap = my_stock['wap'].values.reshape(481,time.size//481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "waps = my_stock.drop(columns=['date_id','seconds_in_bucket']).values.reshape(481,55)\n",
    "waps = waps.transpose()\n",
    "print(waps.shape)\n",
    "# waps = waps[30:]\n",
    "# print(waps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_differences = True\n",
    "if analyze_differences:\n",
    "    for i in range(waps.shape[0]):\n",
    "        waps[i][0] = 0\n",
    "        waps[i][1:] = waps[i][1:] - waps[i][:-1]\n",
    "        \n",
    "#Create 30 point cloud batches\n",
    "stock_100_test_cloud = np.reshape(waps,(55,13,37))\n",
    "stock_100_test_cloud = stock_100_test_cloud.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3947b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "for j in range(55):\n",
    "    plt.plot(range(37),stock_100_test_cloud[i,:,j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37020d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_100_test_cloud[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(stock_100_test_cloud[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0eea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])\n",
    "diagrams_test = VR.fit_transform(stock_100_test_cloud)\n",
    "diagrams_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(diagrams_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import PearsonDissimilarity\n",
    "from gtda.diagrams import Amplitude\n",
    "PD = PearsonDissimilarity()\n",
    "\n",
    "X_pd = PD.fit_transform(stock_100_test_cloud)\n",
    "VR = VietorisRipsPersistence(metric=\"precomputed\",homology_dimensions=[0, 1, 2])\n",
    "X_vr = VR.fit_transform(X_pd)  # \"precomputed\" required on dissimilarity data\n",
    "Ampl = Amplitude()\n",
    "X_a = Ampl.fit_transform(X_vr)\n",
    "X_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a7a06",
   "metadata": {},
   "source": [
    "# Delay embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff469ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import SingleTakensEmbedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4306014",
   "metadata": {},
   "source": [
    "# on trial data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a03d2",
   "metadata": {},
   "source": [
    "### Black Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_BS = 5\n",
    "delay_BS = 2\n",
    "stride = 1\n",
    "\n",
    "embedder_BS = SingleTakensEmbedding(\n",
    "    parameters_type=\"fixed\",\n",
    "    n_jobs=1,\n",
    "    time_delay=delay_BS,\n",
    "    dimension=dimension_BS,\n",
    "    stride=stride,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39392348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#S0 is meaningless for log returns\n",
    "Npaths = 1\n",
    "clouds = 1\n",
    "points_per_cloud_per_path = 100\n",
    "\n",
    "data_BS = stock_path_BS(100, 10, 0.01, 0.02, 0.3, Npaths, clouds*points_per_cloud_per_path,log=True)\n",
    "analyze_differences = True\n",
    "if analyze_differences:\n",
    "    for i in range(ALL_PATHS.shape[0]):\n",
    "        ALL_PATHS[i][0] = 0\n",
    "        ALL_PATHS[i][1:] = ALL_PATHS[i][1:] - ALL_PATHS[i][:-1]\n",
    "        \n",
    "#Create 30 point cloud batches\n",
    "# BS_point_clouds = np.reshape(ALL_PATHS,(Npaths,clouds,points_per_cloud_per_path))\n",
    "# BS_point_clouds = BS_point_clouds.transpose(1,2,0)\n",
    "# BS_point_clouds = BS_point_clouds[:,:,0]\n",
    "embedded_BS = embedder_BS.fit_transform(data_BS[0])\n",
    "print(f\"Shape of embedded time series: {embedded_BS.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(embedded_BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])\n",
    "diagrams_test_delay = VR.fit_transform([embedded_BS])\n",
    "diagrams_test_delay.shape\n",
    "plot_diagram(diagrams_test_delay[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ccd9d",
   "metadata": {},
   "source": [
    "### Heston-Nandi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46fe3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3734077",
   "metadata": {},
   "source": [
    "# on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dimension_periodic = 5\n",
    "embedding_time_delay_periodic = 2\n",
    "stride = 1\n",
    "\n",
    "embedder_periodic = SingleTakensEmbedding(\n",
    "    parameters_type=\"fixed\",\n",
    "    n_jobs=1,\n",
    "    time_delay=embedding_time_delay_periodic,\n",
    "    dimension=embedding_dimension_periodic,\n",
    "    stride=stride,\n",
    ")\n",
    "\n",
    "#upper bounds for search\n",
    "# time_delay=5\n",
    "# embedding_dimension=10\n",
    "\n",
    "# embedder_periodic = SingleTakensEmbedding(\n",
    "#     parameters_type=\"search\", time_delay=time_delay, dimension=embedding_dimension,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9557f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "waps = my_stock.drop(columns=['date_id','seconds_in_bucket']).values.reshape(481,55)\n",
    "data = waps[10]\n",
    "analyze_differences = False\n",
    "if analyze_differences:\n",
    "    data = data[1:]-data[:-1]\n",
    "\n",
    "embedded = embedder_periodic.fit_transform(data)\n",
    "print(f\"Shape of embedded time series: {embedded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])\n",
    "diagrams_test_delay = VR.fit_transform([embedded])\n",
    "diagrams_test_delay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(diagrams_test_delay[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a1c768",
   "metadata": {},
   "source": [
    "# imbalance size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1594cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_and_wap = pd.read_csv('../train.csv',delimiter=',',usecols=['stock_id','date_id','seconds_in_bucket','imbalance_size'])\n",
    "my_stock_IB = id_and_wap[id_and_wap['stock_id']==100].drop(columns='stock_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "IBs = my_stock_IB.drop(columns=['date_id','seconds_in_bucket']).values.reshape(481,55)\n",
    "data = IBs[10]\n",
    "\n",
    "embedding_dimension_periodic = 3\n",
    "embedding_time_delay_periodic = 1\n",
    "stride = 1\n",
    "\n",
    "embedder_periodic = SingleTakensEmbedding(\n",
    "    parameters_type=\"fixed\",\n",
    "    n_jobs=1,\n",
    "    time_delay=embedding_time_delay_periodic,\n",
    "    dimension=embedding_dimension_periodic,\n",
    "    stride=stride,\n",
    ")\n",
    "embedded = embedder_periodic.fit_transform(data)\n",
    "print(f\"Shape of embedded time series: {embedded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89541015",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(data.size),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e25507",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])\n",
    "diagrams_test_delay = VR.fit_transform([embedded])\n",
    "diagrams_test_delay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b65599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(diagrams_test_delay[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b55a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
